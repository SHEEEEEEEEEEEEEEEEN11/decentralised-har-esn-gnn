#!/usr/bin/env python3
import os, math, glob, pickle, argparse, json
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score, log_loss
from sklearn.model_selection import train_test_split
import torch

R_CHOICES = [400, 600, 800, 1200]

def _load_split(split_dir, split_name):
    inertial_dir = os.path.join(split_dir, "Inertial Signals")
    sigs = []
    used = None
    for prefix in ("body_acc", "total_acc"):
        cand = [f"{prefix}_{ax}_{split_name}.txt" for ax in ("x","y","z")]
        if all(os.path.exists(os.path.join(inertial_dir, f)) for f in cand):
            for f in cand:
                sigs.append(np.loadtxt(os.path.join(inertial_dir, f))[...,None])
            used = prefix
            break
    if used is None:
        raise FileNotFoundError(f"No acc files found in {inertial_dir}")
    for ax in ("x","y","z"):
        sigs.append(np.loadtxt(os.path.join(inertial_dir, f"body_gyro_{ax}_{split_name}.txt"))[...,None])
    X = np.concatenate(sigs, axis=2).astype(np.float32)
    y = np.loadtxt(os.path.join(split_dir, f"y_{split_name}.txt")).astype(int) - 1
    return X, y

def load_uci_har_all(data_dir):
    train_dir, test_dir = os.path.join(data_dir,"train"), os.path.join(data_dir,"test")
    X_train, y_train = _load_split(train_dir, "train")
    X_test,  y_test  = _load_split(test_dir,  "test")
    return X_train, y_train, X_test, y_test

def spectral_scale(W, target=0.85, iters=50, seed=42):
    R = W.shape[0]
    rng = np.random.default_rng(seed)
    v = rng.normal(size=(R,)).astype(np.float32)
    v /= (np.linalg.norm(v)+1e-12)
    for _ in range(iters):
        v = W @ v
        n = np.linalg.norm(v)+1e-12
        v /= n
    lam = float(np.linalg.norm(W @ v))
    if lam > 1e-8:
        W *= (target/lam)
    return W

def esn_encode_batch(X, Win, Wres, leak=0.25, washout=32):
    N, T, C = X.shape
    R = Win.shape[0]
    feats = np.empty((N, 3*R), dtype=np.float32)
    for i in range(N):
        h = np.zeros((R,), np.float32)
        H_last=None; H_sum=np.zeros(R, np.float32); H_sq=np.zeros(R, np.float32); cnt=0
        for t in range(T):
            pre = Win @ X[i,t] + Wres @ h
            h = (1.0-leak)*h + leak*np.tanh(pre)
            if t >= washout:
                H_last=h; H_sum+=h; H_sq+=h*h; cnt+=1
        K = max(cnt,1)
        mu = H_sum / K
        std = np.sqrt(np.maximum(H_sq / K - mu**2, 0.0))
        last = H_last if H_last is not None else h
        vec = np.concatenate([last, mu, std])
        feats[i] = vec / (np.linalg.norm(vec)+1e-8)
    return feats

def grid_temperature(logits_val, y_val, grid=np.linspace(0.5, 5.0, 19)):
    z = logits_val - logits_val.max(axis=1, keepdims=True)
    bestT, bestNLL = 1.0, 1e9
    for T in grid:
        p = np.exp(z/T); p /= p.sum(axis=1, keepdims=True)
        nll = log_loss(y_val, p, labels=np.arange(p.shape[1]))
        if nll < bestNLL: bestNLL, bestT = nll, float(T)
    return bestT

def expected_calibration_error(prob_max, correct, M=15):
    bins = np.linspace(0,1,M+1)
    e = 0.0
    N = len(prob_max)
    for b in range(M):
        lo,hi = bins[b], bins[b+1]
        idx = (prob_max >= lo) & (prob_max <= hi) if b==0 else (prob_max > lo) & (prob_max <= hi)
        if idx.sum()==0: continue
        acc  = correct[idx].mean()
        conf = prob_max[idx].mean()
        e += (idx.sum()/N)*abs(acc-conf)
    return float(e)

def build_shared_esn_artefact(data_dir, save_path, seed=42):
    X_tr_full, y_tr_full, X_te, y_te = load_uci_har_all(data_dir)
    X_tr, X_va, y_tr, y_va = train_test_split(X_tr_full, y_tr_full, test_size=0.20, random_state=seed, stratify=y_tr_full)
    mu = X_tr.mean(axis=(0,1), keepdims=True).astype(np.float32)
    sd = (X_tr.std(axis=(0,1), keepdims=True)+1e-8).astype(np.float32)
    norm = lambda X: (X - mu)/sd
    X_tr, X_va, X_te = norm(X_tr), norm(X_va), norm(X_te)
    R_FULL, C = 1200, 6
    rng = np.random.default_rng(seed)
    Win_full  = (rng.normal(0,1,size=(R_FULL,C)).astype(np.float32) * 0.6)
    Wres_full = rng.normal(0,1,size=(R_FULL,R_FULL)).astype(np.float32)
    mask = (rng.random(size=Wres_full.shape) < 0.05); Wres_full *= mask
    Wres_full = spectral_scale(Wres_full, target=0.85, iters=50, seed=seed)
    results = {}
    best_R, best_val = None, -1.0
    for R in R_CHOICES:
        Win = Win_full[:R]; Wres = Wres_full[:R,:R]
        H_tr = esn_encode_batch(X_tr, Win, Wres, leak=0.25, washout=32)
        H_va = esn_encode_batch(X_va, Win, Wres, leak=0.25, washout=32)
        H_te = esn_encode_batch(X_te, Win, Wres, leak=0.25, washout=32)
        sc = StandardScaler().fit(H_tr)
        Z_tr, Z_va, Z_te = sc.transform(H_tr), sc.transform(H_va), sc.transform(H_te)
        pca = PCA(n_components=0.95, svd_solver="full").fit(Z_tr)
        Z_tr, Z_va, Z_te = pca.transform(Z_tr), pca.transform(Z_va), pca.transform(Z_te)
        clf = LogisticRegression(max_iter=12000, solver="lbfgs", C=0.7, multi_class="multinomial", random_state=seed)
        clf.fit(Z_tr, y_tr)
        T_star = grid_temperature(clf.decision_function(Z_va), y_va)
        z = clf.decision_function(Z_va)/max(T_star,1e-8); z -= z.max(axis=1,keepdims=True)
        P = np.exp(z); P /= P.sum(axis=1,keepdims=True)
        vacc = (P.argmax(1)==y_va).mean()
        zt = clf.decision_function(Z_te)/max(T_star,1e-8); zt -= zt.max(axis=1,keepdims=True)
        Pt = np.exp(zt); Pt /= Pt.sum(axis=1,keepdims=True)
        tacc = (Pt.argmax(1)==y_te).mean()
        mf1 = f1_score(y_te, Pt.argmax(1), average="macro")
        ece = expected_calibration_error(Pt.max(1), (Pt.argmax(1)==y_te))
        results[R] = dict(val_acc=float(vacc), test_acc=float(tacc), macro_f1=float(mf1), ece=float(ece),
                          scaler=sc, pca=pca, clf=clf, T=float(T_star), pca_dims=int(Z_va.shape[1]))
        if vacc > best_val: best_val, best_R = vacc, R
    models_by_R = {int(R):{"scaler":results[R]["scaler"],"pca":results[R]["pca"],"clf":results[R]["clf"],"T":float(results[R]["T"])} for R in R_CHOICES}
    artefact = {
        "R_full": R_FULL,
        "best_R": int(best_R),
        "Win_full": Win_full,
        "Wres_full": Wres_full,
        "mu": mu, "sd": sd,
        "config": {"LEAK":0.25,"WASHOUT":32,"SPECTRAL":0.85,"SPARSITY":0.05,"INPUT_SCALE":0.6,
                    "PCA_VARIANCE":0.95,"R_CHOICES":R_CHOICES,"seed":seed},
        "metrics": {"val_by_R": {int(r):results[r]["val_acc"] for r in results},
                    "test_by_R":{int(r):{"acc":results[r]["test_acc"],"macro_f1":results[r]["macro_f1"],
                                         "ece":results[r]["ece"],"pca_dims":results[r]["pca_dims"]} for r in results}},
        "models_by_R": models_by_R
    }
    os.makedirs(os.path.dirname(save_path) or ".", exist_ok=True)
    with open(save_path, "wb") as f:
        pickle.dump(artefact, f)
    return artefact

def load_or_build_artefact(data_dir, artefact_path, seed=42):
    if os.path.exists(artefact_path):
        with open(artefact_path,"rb") as f:
            a = pickle.load(f)
        keys = {"Win_full","Wres_full","best_R","mu","sd"}
        if isinstance(a, dict) and keys.issubset(a.keys()) and ("models_by_R" in a):
            return a
    return build_shared_esn_artefact(data_dir, artefact_path, seed=seed)

class ESNClassifier:
    def __init__(self, a):
        self.a = a
        cfg = a.get("config",{})
        self.leak = cfg.get("LEAK",0.25)
        self.washout = cfg.get("WASHOUT",32)
    def predict_proba(self, window_128x6: np.ndarray, R_used=None):
        if R_used is None:
            R_used = int(self.a["best_R"])
        X = (window_128x6 - self.a["mu"].squeeze()) / self.a["sd"].squeeze()
        Win  = self.a["Win_full"][:R_used]
        Wres = self.a["Wres_full"][:R_used, :R_used]
        h = np.zeros((R_used,), np.float32); states=[]
        for t in range(X.shape[0]):
            pre = Win @ X[t] + Wres @ h
            h = (1.0-self.leak)*h + self.leak*np.tanh(pre)
            if t >= self.washout: states.append(h.copy())
        if len(states)==0:
            feat = np.zeros((3*R_used,), np.float32)
        else:
            S = np.stack(states,0)
            feat = np.concatenate([S[-1], S.mean(0), S.std(0)])
        feat = feat/(np.linalg.norm(feat)+1e-8)
        m = self.a["models_by_R"][int(R_used)]
        Z = m["scaler"].transform(feat.reshape(1,-1))
        Z = m["pca"].transform(Z)
        logits = m["clf"].decision_function(Z)[0]
        T = float(m["T"])
        z = logits / max(T,1e-8); z -= z.max()
        p = np.exp(z); p /= p.sum()
        return p

def load_test_only(data_dir):
    test_dir = os.path.join(data_dir,"test")
    return _load_split(test_dir,"test")

class HarRoutingEnv:
    def __init__(self, X_test, y_test, R_CHOICES, seed=42, batch_size=1, deadline_s=0.25, device_str=None):
        self.X_test, self.y_test = X_test, y_test
        self.N = len(y_test)
        self.bsz = batch_size
        self.D = 6
        self.R_CHOICES = R_CHOICES
        self.deadline_s = deadline_s
        self.rng = np.random.default_rng(seed)
        self.device_str = device_str or ("cuda" if torch.cuda.is_available() else "cpu")
        self.cpu_speed = self.rng.uniform(1.0,2.0,self.D).astype(np.float32)
        self.tx_power  = self.rng.uniform(0.4,0.8,self.D).astype(np.float32)
        self.comp_power= self.rng.uniform(0.8,1.5,self.D).astype(np.float32)
        self.price_w   = self.rng.uniform(0.5,1.5,self.D).astype(np.float32)
        self.ram_cap   = self.rng.uniform(1000,2000,self.D).astype(np.float32)
        self.ram_need  = {400:250,600:400,800:550,1200:800}
        A = np.zeros((self.D,self.D), np.float32)
        for i in range(self.D):
            A[i,i]=1; A[i,(i+1)%self.D]=1; A[i,(i-1)%self.D]=1
        deg = A.sum(1, keepdims=True)
        self.Ahat = A/(deg+1e-8)
        self.reset()
    def reset(self):
        self.t=0.0; self.k=0
        self.queue = np.zeros(self.D, np.float32)
        self.util  = np.zeros(self.D, np.float32)
        self.soc   = np.ones(self.D, np.float32)*0.9
        self.temp  = np.ones(self.D, np.float32)*30.0
        self.hb_age= np.zeros(self.D, np.float32)
        self.tel_age=np.zeros(self.D, np.float32)
        self._refresh_bw()
        self.window_log=[]
        return self._state()
    def _refresh_bw(self):
        self.uplink = np.random.uniform(5,20,self.D).astype(np.float32)
        self.bw_hat = self.uplink * np.random.uniform(0.8,1.2,self.D).astype(np.float32)
    def _state(self):
        device = torch.device(self.device_str)
        F = np.zeros((self.bsz,self.D,9), np.float32)
        for d in range(self.D):
            F[0,d]=[
                self.queue[d], self.util[d], self.soc[d], self.temp[d]/100.0,
                self.bw_hat[d]/20.0, self.hb_age[d]/5.0, self.tel_age[d]/5.0,
                (self.ram_cap[d])/2000.0, self.price_w[d]/2.0
            ]
        run_mask = np.ones((self.bsz,self.D), np.bool_)
        slc_mask = np.ones((self.bsz,self.D,len(self.R_CHOICES)), np.bool_)
        for d in range(self.D):
            if (self.soc[d] <= 0.1) or (self.temp[d] >= 85.0):
                run_mask[0,d]=False
            for ri,r in enumerate(self.R_CHOICES):
                if self.ram_cap[d] < self.ram_need[r]:
                    slc_mask[0,d,ri]=False
        return {
            "A": torch.as_tensor(self.Ahat, dtype=torch.float32, device=device)[None,...].expand(self.bsz,-1,-1),
            "X": torch.as_tensor(F, dtype=torch.float32, device=device),
            "run_mask": torch.as_tensor(run_mask, dtype=torch.bool, device=device),
            "slice_mask": torch.as_tensor(slc_mask, dtype=torch.bool, device=device)
        }
    def step(self, actions, clf, R_used):
        device = torch.device(self.device_str)
        a_run = actions["runner"]
        a_slc = actions["slice"]
        runner = int(a_run[0].item())
        r_idx  = int(a_slc[0].item())
        idx = self.k % self.N
        origin = idx % self.D
        x = self.X_test[idx]; y = int(self.y_test[idx])
        p = clf.predict_proba(x, R_used)
        yhat = int(p.argmax()); correct = float(yhat==y)
        prob_max = float(p.max()); nll = float(-math.log(p[y]+1e-12))
        payload_bits = 128*6*32
        if runner == origin:
            T_tx=0.0; energy_tx=0.0; local=1
        else:
            T_tx = payload_bits/(self.uplink[origin]*1e6)
            energy_tx = float(self.tx_power[origin]*T_tx); local=0
        T_queue = float(self.queue[runner]*0.01)
        base_compute=0.015
        T_compute = float(base_compute*(R_used/400.0)/max(self.cpu_speed[runner],1e-6))
        energy_compute = float(self.comp_power[runner]*T_compute)
        T_total = T_tx + T_queue + T_compute
        deadline_miss = int(T_total > self.deadline_s)
        price_cost = float(self.price_w[runner]*(T_compute + 0.5*T_tx*(1-local)))
        LAMBDA_T, LAMBDA_E, LAMBDA_PI = 0.5, 0.2, 0.2
        T0, E0 = 0.20, 0.20
        r_acc = correct
        r_lat = -LAMBDA_T*(T_total/T0)
        r_eng = -LAMBDA_E*((energy_tx+energy_compute)/E0)
        r_price = -LAMBDA_PI*self.price_w[runner]
        reward = float(r_acc + r_lat + r_eng + r_price)
        self.window_log.append({
            "tstamp": self.t,
            "origin_id": origin,
            "device": runner,
            "slice_R": R_used,
            "mask_applied": 0,
            "mask_reason": "",
            "hb_age_origin": float(self.hb_age[origin]),
            "hb_age_runner": float(self.hb_age[runner]),
            "tel_age_origin": float(self.tel_age[origin]),
            "tel_age_runner": float(self.tel_age[runner]),
            "uplink_origin": float(self.uplink[origin]),
            "downlink_runner": float(self.uplink[runner]),
            "b_hat": float(self.bw_hat[origin]),
            "residual_bw": float(max(0.0, self.uplink[origin] - payload_bits/1e6)),
            "bandwidth_err": float(abs(self.uplink[origin]-self.bw_hat[origin])),
            "payload_bytes": float(payload_bits/8),
            "T_tx": float(T_tx),
            "T_queue": float(T_queue),
            "T_compute": float(T_compute),
            "T_total": float(T_total),
            "deadline_miss": int(deadline_miss),
            "energy_tx": float(energy_tx),
            "energy_compute": float(energy_compute),
            "price_cost": float(price_cost),
            "queue_len_before": float(self.queue[runner]),
            "queue_len_after": float(self.queue[runner]*0.9 + 1.0),
            "util_before": float(self.util[runner]),
            "pred_label": int(yhat),
            "true_label": int(y),
            "prob_max": float(prob_max),
            "nll": float(nll),
            "reward": float(reward),
            "r_acc": float(r_acc),
            "r_lat": float(r_lat),
            "r_eng": float(r_eng),
            "r_price": float(r_price),
            "local": int(local),
        })
        self.t += 0.05; self.k += 1
        self.queue[runner] = self.queue[runner]*0.9 + np.random.uniform(0.5,1.5)
        self.util = 0.95*self.util; self.util[runner] += 0.05
        self.soc[runner] = max(0.0, self.soc[runner]-0.001)
        self.temp[runner] = min(85.0, self.temp[runner]+0.2)
        self.temp = 30.0 + (self.temp-30.0)*0.99
        self.hb_age += 0.05; self.tel_age += 0.05
        if np.random.rand()<0.1:
            self.hb_age[np.random.randint(self.D)] = 0.0
            self.tel_age[np.random.randint(self.D)] = 0.0
            self._refresh_bw()
        done = (self.k >= self.N)
        return self._state(), torch.as_tensor([reward], dtype=torch.float32, device=device), bool(done)

def evaluate_greedy_static(env, max_steps, R_star):
    state = env.reset()
    assert R_star in R_CHOICES
    r_idx  = R_CHOICES.index(R_star)
    steps = min(max_steps, env.N)
    device = torch.device(env.device_str)
    for _ in range(steps):
        mask = state["run_mask"][0].detach().cpu().numpy()
        feasible = np.where(mask)[0]
        if len(feasible)==0:
            d_star = int(np.argmax(env.cpu_speed))
        else:
            d_star = int(feasible[np.argmax(env.cpu_speed[feasible])])
        a_run = torch.as_tensor([d_star], dtype=torch.long, device=device)
        a_slc = torch.as_tensor([r_idx], dtype=torch.long, device=device)
        state, _, done = env.step({"runner": a_run, "slice": a_slc}, clf, R_star)
        if done: break

def compute_metrics(df: pd.DataFrame):
    acc = (df.pred_label==df.true_label).mean()
    macro_f1 = f1_score(df.true_label, df.pred_label, average="macro")
    nll = df.nll.mean()
    ece = expected_calibration_error(df.prob_max.values, (df.pred_label.values==df.true_label.values))
    total_reward = df.reward.sum()
    lat_p95 = df.T_total.quantile(0.95)
    throughput = len(df)/df.T_total.sum()
    miss_rate = df.deadline_miss.mean()
    e_per = (df.energy_tx+df.energy_compute).mean()
    price_per = df.price_cost.mean()
    counts = df.device.value_counts().values
    jain = (counts.sum()**2)/(len(counts)*(counts**2).sum()) if len(counts)>0 else float('nan')
    return {
        "accuracy":float(acc), "macro_f1":float(macro_f1), "nll":float(nll), "ece":float(ece),
        "total_reward":float(total_reward), "lat_p95_ms": float(lat_p95*1000.0),
        "throughput": float(throughput), "deadline_miss_rate": float(miss_rate),
        "energy_per": float(e_per), "price_per": float(price_per), "jain": float(jain)
    }

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--data_dir", type=str, default="./data/UCI_HAR_Dataset")
    ap.add_argument("--artefact_path", type=str, default="artifacts/shared_esn.pkl")
    ap.add_argument("--out_dir", type=str, default="results/greedy")
    ap.add_argument("--max_steps", type=int, default=3000)
    ap.add_argument("--seed", type=int, default=42)
    args = ap.parse_args()

    os.makedirs(args.out_dir, exist_ok=True)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)

    artefact = load_or_build_artefact(args.data_dir, args.artefact_path, seed=args.seed)
    X_train, y_train, X_test, y_test = load_uci_har_all(args.data_dir)
    global clf
    clf = ESNClassifier(artefact)
    env = HarRoutingEnv(X_test, y_test, R_CHOICES=R_CHOICES, seed=args.seed, batch_size=1, deadline_s=0.25)
    R_star = int(artefact["best_R"])
    evaluate_greedy_static(env, max_steps=args.max_steps, R_star=R_star)
    df = pd.DataFrame(env.window_log)
    df["policy"] = "greedy_static"
    csv_path = os.path.join(args.out_dir, "window_log_greedy.csv")
    df.to_csv(csv_path, index=False)
    metrics = compute_metrics(df)
    with open(os.path.join(args.out_dir, "metrics.json"), "w") as f:
        json.dump(metrics, f, indent=2)
    print(csv_path)
    print(os.path.join(args.out_dir, "metrics.json"))

if __name__ == "__main__":
    main()

