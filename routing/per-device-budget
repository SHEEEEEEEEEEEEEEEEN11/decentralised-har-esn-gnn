#!/usr/bin/env python3
# Per-device HAR routing (GCN+PPO)
# Train = stochastic PPO (accuracy + latency + energy + price)
# Eval  = budget-aware heuristic: latency <= budget first, then min price → min latency → min energy → min device id

import os, pickle, argparse, warnings, math, random
import numpy as np
import pandas as pd
from collections import defaultdict, deque

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score, log_loss
from sklearn.model_selection import train_test_split

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Categorical

warnings.filterwarnings("ignore")
R_CHOICES = [400, 600, 800, 1200]

# ---------------- Data ----------------
def _load_split(split_dir, split_name):
    inertial_dir = os.path.join(split_dir, "Inertial Signals")
    sigs = []; used=None
    for prefix in ("body_acc", "total_acc"):
        cand = [f"{prefix}_{ax}_{split_name}.txt" for ax in ("x","y","z")]
        if all(os.path.exists(os.path.join(inertial_dir, f)) for f in cand):
            for f in cand: sigs.append(np.loadtxt(os.path.join(inertial_dir, f))[...,None])
            used = prefix; break
    if used is None: raise FileNotFoundError(f"No acc files found in {inertial_dir}")
    for ax in ("x","y","z"):
        sigs.append(np.loadtxt(os.path.join(inertial_dir, f"body_gyro_{ax}_{split_name}.txt"))[...,None])
    X = np.concatenate(sigs, axis=2).astype(np.float32)  # (N, 128, 6)
    y = np.loadtxt(os.path.join(split_dir, f"y_{split_name}.txt")).astype(int) - 1
    return X, y

def load_uci_har_all(data_dir):
    tr = os.path.join(data_dir,"train"); te = os.path.join(data_dir,"test")
    Xtr, ytr = _load_split(tr, "train")
    Xte, yte = _load_split(te, "test")
    return Xtr, ytr, Xte, yte

# ---------------- ESN utils ----------------
def spectral_scale(W, target=0.85, iters=50, seed=42):
    R = W.shape[0]
    rng = np.random.default_rng(seed)
    v = rng.normal(size=(R,)).astype(np.float32); v /= (np.linalg.norm(v)+1e-12)
    for _ in range(iters):
        v = W @ v; n = np.linalg.norm(v)+1e-12; v /= n
    lam = float(np.linalg.norm(W @ v))
    if lam > 1e-8: W *= (target/lam)
    return W

def esn_encode_batch(X, Win, Wres, leak=0.25, washout=32):
    N, T, C = X.shape; R = Win.shape[0]
    feats = np.empty((N, 3*R), dtype=np.float32)
    for i in range(N):
        h = np.zeros((R,), np.float32)
        H_last=None; H_sum=np.zeros(R, np.float32); H_sq=np.zeros(R, np.float32); cnt=0
        for t in range(T):
            pre = Win @ X[i,t] + Wres @ h
            h = (1.0-leak)*h + leak*np.tanh(pre)
            if t >= washout:
                H_last=h; H_sum+=h; H_sq+=h*h; cnt+=1
        K = max(cnt,1)
        mu = H_sum / K
        std = np.sqrt(np.maximum(H_sq / K - mu**2, 0.0))
        last = H_last if H_last is not None else h
        vec = np.concatenate([last, mu, std])
        feats[i] = vec / (np.linalg.norm(vec)+1e-8)
    return feats

def grid_temperature(logits_val, y_val, grid=np.linspace(0.5, 5.0, 19)):
    z = logits_val - logits_val.max(axis=1, keepdims=True)
    bestT, bestNLL = 1.0, 1e9
    for T in grid:
        p = np.exp(z/T); p /= p.sum(axis=1, keepdims=True)
        nll = log_loss(y_val, p, labels=np.arange(p.shape[1]))
        if nll < bestNLL: bestNLL, bestT = nll, float(T)
    return bestT

def expected_calibration_error(prob_max, correct, M=15):
    bins = np.linspace(0,1,M+1); e=0.0; N=len(prob_max)
    for b in range(M):
        lo,hi = bins[b], bins[b+1]
        idx = (prob_max > lo) & (prob_max <= hi) if b>0 else (prob_max >= lo) & (prob_max <= hi)
        if idx.sum()==0: continue
        acc  = correct[idx].mean(); conf = prob_max[idx].mean()
        e += (idx.sum()/N)*abs(acc-conf)
    return float(e)

# ---------------- Dirichlet split ----------------
def dirichlet_partition_indices(y, n_devices, alpha, seed=42):
    rng = np.random.default_rng(seed)
    per_dev = [[] for _ in range(n_devices)]
    for c in np.unique(y):
        idx_c = np.where(y==c)[0]
        rng.shuffle(idx_c)
        probs = rng.dirichlet([alpha]*n_devices)
        counts = np.random.multinomial(len(idx_c), probs)
        s=0
        for d,k in enumerate(counts):
            if k>0: per_dev[d].extend(idx_c[s:s+k]); s+=k
    return [np.array(v, dtype=int) for v in per_dev]

# ---------------- Artefact ----------------
def artefact_perdev_is_valid(a, n_devices):
    try:
        if a.get("D") != n_devices: return False
        if "device_models" not in a or len(a["device_models"]) != n_devices: return False
        for dm in a["device_models"]:
            if not all(k in dm for k in ("Win_full","Wres_full","mu","sd","best_R","perR")): return False
            if not all(r in dm["perR"] for r in R_CHOICES): return False
            for r in R_CHOICES:
                if not all(k in dm["perR"][r] for k in ("scaler","pca","clf","T")): return False
        if "test_assignment" not in a: return False
        return True
    except Exception:
        return False

def build_per_device_artefact(data_dir, save_path, n_devices=6, alpha=5.0, seed=42):
    print("[per-device] building artefact…")
    Xtr, ytr, Xte, yte = load_uci_har_all(data_dir)
    tr_idx = dirichlet_partition_indices(ytr, n_devices, alpha, seed=seed)
    te_idx = dirichlet_partition_indices(yte, n_devices, alpha, seed=seed+1)
    device_models=[]; results_val=defaultdict(dict); results_test=defaultdict(dict)
    R_FULL, C = 1200, 6
    for d in range(n_devices):
        print(f"[device {d}] ESN→LR per R…")
        idx_d = tr_idx[d]
        Xd, yd = Xtr[idx_d], ytr[idx_d]
        Xd_tr, Xd_va, yd_tr, yd_va = train_test_split(Xd, yd, test_size=0.20, random_state=seed, stratify=yd)
        mu_d = Xd_tr.mean(axis=(0,1), keepdims=True).astype(np.float32)
        sd_d = (Xd_tr.std(axis=(0,1), keepdims=True)+1e-8).astype(np.float32)
        norm = lambda X: (X - mu_d)/sd_d
        Xd_tr, Xd_va = norm(Xd_tr), norm(Xd_va)
        rng = np.random.default_rng(seed + d)
        Win_full  = (rng.normal(0,1,size=(R_FULL,C)).astype(np.float32) * 0.6)
        Wres_full = rng.normal(0,1,size=(R_FULL,R_FULL)).astype(np.float32)
        Wres_full *= (rng.random(size=Wres_full.shape) < 0.05)
        Wres_full = spectral_scale(Wres_full, target=0.85, iters=50, seed=seed+d)
        perR={}; best_R=None; best_val=-1.0
        te_idx_d = te_idx[d]
        Xd_te_loc = ((Xte[te_idx_d] - mu_d)/sd_d).astype(np.float32)
        yd_te_loc = yte[te_idx_d]
        for R in R_CHOICES:
            Win = Win_full[:R]; Wres = Wres_full[:R,:R]
            H_tr = esn_encode_batch(Xd_tr, Win, Wres)
            H_va = esn_encode_batch(Xd_va, Win, Wres)
            sc = StandardScaler().fit(H_tr)
            Z_tr, Z_va = sc.transform(H_tr), sc.transform(H_va)
            pca = PCA(n_components=0.95, svd_solver="full").fit(Z_tr)
            Z_tr, Z_va = pca.transform(Z_tr), pca.transform(Z_va)
            clf = LogisticRegression(max_iter=12000, solver="lbfgs", C=0.7, multi_class="multinomial", random_state=seed)
            clf.fit(Z_tr, yd_tr)
            T_star = grid_temperature(clf.decision_function(Z_va), yd_va)
            zv = clf.decision_function(Z_va)/max(T_star,1e-8); zv -= zv.max(axis=1,keepdims=True)
            Pv = np.exp(zv); Pv /= Pv.sum(axis=1,keepdims=True)
            vacc = float((Pv.argmax(1)==yd_va).mean())
            if len(Xd_te_loc)>0:
                H_te = esn_encode_batch(Xd_te_loc, Win, Wres)
                Z_te = pca.transform(sc.transform(H_te))
                zt = clf.decision_function(Z_te)/max(T_star,1e-8); zt -= zt.max(axis=1,keepdims=True)
                Pt = np.exp(zt); Pt /= Pt.sum(axis=1,keepdims=True)
                tacc = float((Pt.argmax(1)==yd_te_loc).mean())
                mf1  = float(f1_score(yd_te_loc, Pt.argmax(1), average="macro")) if len(np.unique(yd_te_loc))>1 else float("nan")
                ece  = float(expected_calibration_error(Pt.max(1), (Pt.argmax(1)==yd_te_loc)))
            else:
                tacc = mf1 = ece = float("nan")
            results_val[d][R]  = vacc
            results_test[d][R] = {"acc": tacc, "macro_f1": mf1, "ece": ece, "pca_dims": int(Z_va.shape[1])}
            perR[R] = {"scaler": sc, "pca": pca, "clf": clf, "T": float(T_star)}
            if vacc > best_val: best_val, best_R = vacc, R
            print(f"  R={R}: val={vacc:.3f} test_acc={tacc:.3f}")
        device_models.append({"Win_full":Win_full,"Wres_full":Wres_full,"perR":perR,"best_R":int(best_R),"mu":mu_d,"sd":sd_d})
    artefact = {
        "type":"per_device","alpha":alpha,"D":n_devices,"R_CHOICES":R_CHOICES,
        "device_models":device_models,"val_by_device":results_val,
        "test_by_device":results_test,"test_assignment":[arr.tolist() for arr in te_idx]
    }
    os.makedirs(os.path.dirname(save_path) or ".", exist_ok=True)
    with open(save_path,"wb") as f: pickle.dump(artefact,f)
    print(f"[per-device] saved -> {save_path}")
    return artefact

# ---------------- Classifier wrapper ----------------
class PerDeviceClassifier:
    def __init__(self, artefact):
        self.dev_models = artefact["device_models"]; self.leak=0.25; self.washout=32
    def predict_proba(self, window_128x6, runner_d, R_used):
        dm = self.dev_models[runner_d]
        X = (window_128x6 - dm["mu"].squeeze()) / dm["sd"].squeeze()
        Win = dm["Win_full"][:R_used]; Wres = dm["Wres_full"][:R_used,:R_used]
        h = np.zeros((R_used,), np.float32); states=[]
        for t in range(X.shape[0]):
            pre = Win @ X[t] + Wres @ h
            h = (1.0-self.leak)*h + self.leak*np.tanh(pre)
            if t >= self.washout: states.append(h.copy())
        feat = np.zeros((3*R_used,), np.float32) if len(states)==0 else np.concatenate([states[-1], np.mean(states,0), np.std(states,0)])
        feat = feat/(np.linalg.norm(feat)+1e-8)
        perR = dm["perR"][R_used]
        Z = perR["scaler"].transform(feat.reshape(1,-1))
        Z = perR["pca"].transform(Z)
        logits = perR["clf"].decision_function(Z)[0]
        T = float(perR["T"])
        z = logits / max(T,1e-8); z -= z.max()
        p = np.exp(z); p /= p.sum()
        return p

# ---------------- Env ----------------
class HarRoutingEnvPerDev:
    def __init__(self, origin_seq, X_test, y_test, clf, deadline_s=0.25, seed=42,
                 lambda_t=0.5, lambda_e=0.2, lambda_pi=0.2, T0=0.20, E0=0.20):
        self.origin_seq = origin_seq
        self.X_test, self.y_test = X_test, y_test
        self.N = len(origin_seq)
        self.D = int(np.max(origin_seq[:,0]))+1 if len(origin_seq)>0 else 0
        self.R_CHOICES = R_CHOICES
        self.deadline_s = deadline_s
        self.lambda_t, self.lambda_e, self.lambda_pi = lambda_t, lambda_e, lambda_pi
        self.T0, self.E0 = T0, E0
        self.clf = clf
        rng = np.random.default_rng(seed)
        self.cpu_speed = rng.uniform(1.0,2.0,self.D).astype(np.float32)
        self.tx_power  = rng.uniform(0.4,0.8,self.D).astype(np.float32)
        self.comp_power= rng.uniform(0.8,1.5,self.D).astype(np.float32)
        self.price_w   = rng.uniform(0.5,1.5,self.D).astype(np.float32)
        self.ram_cap   = rng.uniform(1000,2000,self.D).astype(np.float32)
        self.ram_need  = {400:250,600:400,800:550,1200:800}
        A = np.zeros((self.D,self.D), np.float32)
        for i in range(self.D):
            A[i,i]=1; A[i,(i+1)%self.D]=1; A[i,(i-1)%self.D]=1
        deg = A.sum(1, keepdims=True); self.Ahat = A/(deg+1e-8)
        self.reset()
    def reset(self):
        self.t=0.0; self.k=0
        self.queue = np.zeros(self.D, np.float32)
        self.util  = np.zeros(self.D, np.float32)
        self.soc   = np.ones(self.D, np.float32)*0.9
        self.temp  = np.ones(self.D, np.float32)*30.0
        self.hb_age= np.zeros(self.D, np.float32)
        self.tel_age=np.zeros(self.D, np.float32)
        self._refresh_bw()
        self.window_log=[]
        return self._state()
    def _refresh_bw(self):
        self.uplink = np.random.uniform(5,20,self.D).astype(np.float32)  # Mb/s
        self.bw_hat = self.uplink * np.random.uniform(0.8,1.2,self.D).astype(np.float32)
    def _state(self):
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        F = np.zeros((1,self.D,9), np.float32)
        for d in range(self.D):
            F[0,d]=[self.queue[d], self.util[d], self.soc[d], self.temp[d]/100.0,
                    self.bw_hat[d]/20.0, self.hb_age[d]/5.0, self.tel_age[d]/5.0,
                    (self.ram_cap[d])/2000.0, self.price_w[d]/2.0]
        run_mask = np.ones((1,self.D), np.bool_)
        slc_mask = np.ones((1,self.D,len(self.R_CHOICES)), np.bool_)
        for d in range(self.D):
            if (self.soc[d] <= 0.1) or (self.temp[d] >= 85.0): run_mask[0,d]=False
            for ri,r in enumerate(self.R_CHOICES):
                if self.ram_cap[d] < self.ram_need[r]: slc_mask[0,d,ri]=False
        return {"A": torch.as_tensor(self.Ahat, dtype=torch.float32, device=device)[None,...],
                "X": torch.as_tensor(F, dtype=torch.float32, device=device),
                "run_mask": torch.as_tensor(run_mask, dtype=torch.bool, device=device),
                "slice_mask": torch.as_tensor(slc_mask, dtype=torch.bool, device=device)}
    def step(self, actions):
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        a_run = actions["runner"]; a_slc = actions["slice"]
        runner = int(a_run[0].item()); r_idx = int(a_slc[0].item())
        R_used = self.R_CHOICES[r_idx]
        origin, idx = self.origin_seq[self.k]; origin=int(origin); idx=int(idx)
        x = self.X_test[idx]; y = int(self.y_test[idx])
        p = self.clf.predict_proba(x, runner, R_used)
        yhat = int(p.argmax()); correct = float(yhat==y)
        prob_max = float(p.max()); nll = float(-math.log(p[y]+1e-12))
        payload_bits = 128*6*32
        if runner == origin:
            T_tx=0.0; energy_tx=0.0; local=1
        else:
            T_tx = payload_bits/(self.uplink[origin]*1e6)
            energy_tx = float(self.tx_power[origin]*T_tx); local=0
        T_queue = float(self.queue[runner]*0.01)
        T_compute = float(0.015*(R_used/400.0)/max(self.cpu_speed[runner],1e-6))
        energy_compute = float(self.comp_power[runner]*T_compute)
        T_total = T_tx + T_queue + T_compute
        deadline_miss = int(T_total > self.deadline_s)
        price_cost = float(self.price_w[runner]*(T_compute + 0.5*T_tx*(1-local)))
        r_acc = correct
        r_lat = - self.lambda_t*(T_total/self.T0)
        r_eng = - self.lambda_e*((energy_tx+energy_compute)/self.E0)
        r_price = - self.lambda_pi*self.price_w[runner]
        reward = float(r_acc + r_lat + r_eng + r_price)
        self.window_log.append({
            "tstamp": self.t, "origin_id": origin, "device": runner, "slice_R": R_used,
            "mask_applied": 0, "mask_reason": "",
            "hb_age_origin": float(self.hb_age[origin]), "hb_age_runner": float(self.hb_age[runner]),
            "tel_age_origin": float(self.tel_age[origin]), "tel_age_runner": float(self.tel_age[runner]),
            "uplink_origin": float(self.uplink[origin]), "downlink_runner": float(self.uplink[runner]),
            "b_hat": float(self.bw_hat[origin]),
            "residual_bw": float(max(0.0, self.uplink[origin] - payload_bits/1e6)),
            "bandwidth_err": float(abs(self.uplink[origin]-self.bw_hat[origin])),
            "payload_bytes": float(payload_bits/8),
            "T_tx": float(T_tx), "T_queue": float(T_queue), "T_compute": float(T_compute),
            "T_total": float(T_total), "deadline_miss": int(deadline_miss),
            "energy_tx": float(energy_tx), "energy_compute": float(energy_compute),
            "price_cost": float(price_cost),
            "queue_len_before": float(self.queue[runner]),
            "queue_len_after": float(self.queue[runner]*0.9 + 1.0),
            "util_before": float(self.util[runner]),
            "pred_label": int(yhat), "true_label": int(y),
            "prob_max": float(prob_max), "nll": float(nll),
            "reward": float(reward), "r_acc": float(r_acc),
            "r_lat": float(r_lat), "r_eng": float(r_eng), "r_price": float(r_price),
            "local": int(local),
        })
        # dynamics
        self.t += 0.05; self.k += 1
        self.queue[runner] = self.queue[runner]*0.9 + np.random.uniform(0.5,1.5)
        self.util = 0.95*self.util; self.util[runner] += 0.05
        self.soc[runner] = max(0.0, self.soc[runner]-0.001)
        self.temp[runner] = min(85.0, self.temp[runner]+0.2)
        self.temp = 30.0 + (self.temp-30.0)*0.99
        self.hb_age += 0.05; self.tel_age += 0.05
        if np.random.rand()<0.1:
            self.hb_age[np.random.randint(self.D)] = 0.0
            self.tel_age[np.random.randint(self.D)] = 0.0
            self._refresh_bw()
        done = (self.k >= self.N)
        return self._state(), torch.as_tensor([reward], dtype=torch.float32, device=device), bool(done)

# ---------------- Router ----------------
class RouterGCN(nn.Module):
    def __init__(self, in_dim=9, hidden=64, n_devices=6, n_slices=len(R_CHOICES)):
        super().__init__()
        self.gc1 = nn.Linear(in_dim, hidden)
        self.gc2 = nn.Linear(hidden, hidden)
        self.ln1 = nn.LayerNorm(hidden); self.ln2 = nn.LayerNorm(hidden)
        self.run_head   = nn.Linear(hidden, 1)
        self.slice_head = nn.Linear(hidden, n_slices)
        self.value_head = nn.Linear(hidden, 1)
    def forward(self, A, X):
        H = torch.bmm(A, self.gc1(X)); H = F.relu(self.ln1(H))
        H = torch.bmm(A, self.gc2(H)); H = F.relu(self.ln2(H))
        run_logits   = self.run_head(H).squeeze(-1)
        slice_logits = self.slice_head(H)
        value = self.value_head(H.mean(dim=1)).squeeze(-1)
        return run_logits, slice_logits, value

# ---------------- PPO helpers ----------------
def masked_categorical(logits, mask_bool):
    very_neg = torch.finfo(logits.dtype).min
    logits_masked = torch.where(mask_bool, logits, torch.full_like(logits, very_neg))
    return Categorical(logits=logits_masked)

def sample_action(model, state, device):
    run_logits, slice_logits, value = model(state["A"], state["X"])
    run_dist = masked_categorical(run_logits, state["run_mask"])
    a_run = run_dist.sample()
    slc_logits_sel = slice_logits[torch.arange(a_run.shape[0], device=device), a_run, :]
    slc_mask_sel   = state["slice_mask"][torch.arange(a_run.shape[0], device=device), a_run, :]
    slice_dist = masked_categorical(slc_logits_sel, slc_mask_sel)
    a_slc = slice_dist.sample()
    logp = run_dist.log_prob(a_run) + slice_dist.log_prob(a_slc)
    return a_run, a_slc, logp, value

def compute_gae(rew_t, val_t, v_last, done_t, gamma=0.99, lam=0.95):
    T, B = rew_t.shape
    adv = torch.zeros_like(rew_t); lastgaelam = torch.zeros(B, device=rew_t.device)
    for t in reversed(range(T)):
        nonterm = 1.0 - done_t[t].float()
        nextv = v_last if t==T-1 else val_t[t+1]
        delta = rew_t[t] + gamma*nonterm*nextv - val_t[t]
        lastgaelam = delta + gamma*lam*nonterm*lastgaelam
        adv[t] = lastgaelam
    ret = adv + val_t
    adv = (adv - adv.mean()) / (adv.std(unbiased=False)+1e-8)
    return adv, ret

def flat(x):
    T,B = x.shape[:2]; return x.reshape(T*B, *x.shape[2:])

# ---------------- Train ----------------
def train_router(env, model, updates=10, rollout_T=256, lr=3e-4):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    opt = torch.optim.Adam(model.parameters(), lr=lr)
    logs=[]; state = env.reset()
    for up in range(1, updates+1):
        A_buf=[]; X_buf=[]; runM_buf=[]; slcM_buf=[]; a_run_buf=[]; a_slc_buf=[]; logp_old_buf=[]; val_buf=[]; rew_buf=[]; done_buf=[]
        for _ in range(rollout_T):
            with torch.no_grad():
                a_run, a_slc, logp, value = sample_action(model, state, device)
            next_state, reward, done = env.step({"runner":a_run, "slice":a_slc})
            A_buf.append(state["A"]); X_buf.append(state["X"])
            runM_buf.append(state["run_mask"]); slcM_buf.append(state["slice_mask"])
            a_run_buf.append(a_run); a_slc_buf.append(a_slc)
            logp_old_buf.append(logp); val_buf.append(value)
            rew_buf.append(reward.view(-1))
            done_buf.append(torch.as_tensor([done], dtype=torch.bool, device=device))
            state = next_state
            if done: state = env.reset()
        with torch.no_grad():
            _, _, v_last = model(state["A"], state["X"])
        A_t=torch.stack(A_buf,0); X_t=torch.stack(X_buf,0)
        runM_t=torch.stack(runM_buf,0); slcM_t=torch.stack(slcM_buf,0)
        a_run_t=torch.stack(a_run_buf,0); a_slc_t=torch.stack(a_slc_buf,0)
        logp_old_t=torch.stack(logp_old_buf,0); val_t=torch.stack(val_buf,0)
        rew_t=torch.stack(rew_buf,0); done_t=torch.stack(done_buf,0)
        adv, ret = compute_gae(rew_t, val_t, v_last, done_t)
        adv_mean, adv_std = float(adv.mean().cpu()), float(adv.std(unbiased=False).cpu())
        N = a_run_t.numel()
        A_mb=flat(A_t); X_mb=flat(X_t)
        runM_mb=flat(runM_t); slcM_mb=flat(slcM_t)
        a_run_mb=flat(a_run_t).long(); a_slc_mb=flat(a_slc_t).long()
        adv_mb=flat(adv); ret_mb=flat(ret); logp_old_mb=flat(logp_old_t)
        clip_eps, c_vf, c_ent = 0.2, 0.5, 0.01
        lossp, lossv, entv, klv = [], [], [], []
        epochs=4; mb_size=min(2048,N)
        for _ in range(epochs):
            perm = torch.randperm(N, device=device)
            for s in range(0,N,mb_size):
                idx = perm[s:s+mb_size]
                run_logits, slice_logits, v_pred = model(A_mb[idx], X_mb[idx])
                v_pred = v_pred.squeeze(-1)
                run_dist = masked_categorical(run_logits, runM_mb[idx])
                logp_run = run_dist.log_prob(a_run_mb[idx])
                m = idx.shape[0]; sel = torch.arange(m, device=device)
                slc_logits_sel = slice_logits[sel, a_run_mb[idx], :]
                slc_mask_sel   = slcM_mb[idx][sel, a_run_mb[idx], :]
                slice_dist = masked_categorical(slc_logits_sel, slc_mask_sel)
                logp_slice = slice_dist.log_prob(a_slc_mb[idx])
                new_logp = logp_run + logp_slice
                ratio = torch.exp(new_logp - logp_old_mb[idx])
                surr1 = ratio * adv_mb[idx]; surr2 = torch.clamp(ratio, 1.0-clip_eps, 1.0+clip_eps) * adv_mb[idx]
                policy_loss = -torch.mean(torch.min(surr1, surr2))
                value_loss = 0.5 * F.mse_loss(v_pred, ret_mb[idx])
                entropy = run_dist.entropy().mean() + slice_dist.entropy().mean()
                approx_kl = torch.mean((logp_old_mb[idx] - new_logp)) * -1.0
                loss = policy_loss + c_vf*value_loss - c_ent*entropy
                opt.zero_grad(set_to_none=True); loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)
                opt.step()
                lossp.append(float(policy_loss.detach().cpu())); lossv.append(float(value_loss.detach().cpu()))
                entv.append(float(entropy.detach().cpu())); klv.append(float(approx_kl.detach().cpu()))
        logs.append({"update":up,"adv_mean":adv_mean,"adv_std":adv_std,"policy_loss":np.mean(lossp),
                     "value_loss":np.mean(lossv),"entropy":np.mean(entv),"kl":np.mean(klv),"steps":rollout_T})
        print(f"[update {up}/{updates}] pol={logs[-1]['policy_loss']:.4f} val={logs[-1]['value_loss']:.4f} ent={logs[-1]['entropy']:.3f} kl={logs[-1]['kl']:.4f} advμ={adv_mean:.3f} advσ={adv_std:.3f}")
    return pd.DataFrame(logs)

# ---------------- Eval: budget-aware heuristic ----------------
def evaluate_budget_aware(env, latency_budget=None, max_steps=None):
    state = env.reset()
    steps = env.N if max_steps is None else min(max_steps, env.N)
    if latency_budget is None: latency_budget = env.deadline_s
    payload_bits = 128*6*32
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    for _ in range(steps):
        run_mask  = state["run_mask"][0].detach().cpu().numpy()
        slice_mask= state["slice_mask"][0].detach().cpu().numpy()
        origin, _ = env.origin_seq[env.k]; origin=int(origin)
        cands=[]
        for d in range(env.D):
            if not run_mask[d]: continue
            for ri,R in enumerate(env.R_CHOICES):
                if not slice_mask[d,ri]: continue
                if d==origin: T_tx, local, e_tx = 0.0,1,0.0
                else:
                    T_tx = payload_bits/(env.uplink[origin]*1e6)
                    local=0; e_tx=float(env.tx_power[origin]*T_tx)
                T_queue = float(env.queue[d]*0.01)
                T_compute = float(0.015*(R/400.0)/max(env.cpu_speed[d],1e-6))
                T_total = T_tx + T_queue + T_compute
                e_compute = float(env.comp_power[d]*T_compute)
                price = float(env.price_w[d]*(T_compute + 0.5*T_tx*(1-local)))
                cands.append((T_total, price, e_tx+e_compute, d, ri))
        if not cands:
            a_run=torch.tensor([0], device=device); a_slc=torch.tensor([0], device=device)
        else:
            in_budget=[c for c in cands if c[0] <= latency_budget]
            best = min(in_budget if in_budget else cands, key=lambda t: (t[1], t[0], t[2], t[3]) if in_budget else (t[0], t[1], t[2], t[3]))
            _,_,_, db, rib = best
            a_run=torch.tensor([db], device=device); a_slc=torch.tensor([rib], device=device)
        state, _, done = env.step({"runner":a_run, "slice":a_slc})
        if done: break
    return pd.DataFrame(env.window_log)

# ---------------- Metrics ----------------
def compute_metrics(df: pd.DataFrame):
    acc = (df.pred_label==df.true_label).mean()
    macro_f1 = f1_score(df.true_label, df.pred_label, average="macro")
    nll = df.nll.mean()
    ece = expected_calibration_error(df.prob_max.values, (df.pred_label.values==df.true_label.values))
    total_reward = df.reward.sum()
    lat_med = df.T_total.median(); lat_p95 = df.T_total.quantile(0.95)
    tx_med  = df.T_tx.median();    tx_p95  = df.T_tx.quantile(0.95)
    throughput = len(df)/df.T_total.sum()
    miss_rate = df.deadline_miss.mean()
    e_per = (df.energy_tx+df.energy_compute).mean()
    price_per = df.price_cost.mean()
    r_per_cost = total_reward/(df.price_cost.sum()+1e-8)
    local_rate = df.local.mean()
    dev_dist = df.device.value_counts(normalize=True).sort_index().to_dict()
    slice_dist = df.slice_R.value_counts(normalize=True).sort_index().to_dict()
    feas_rate = 1 - df.mask_applied.mean()
    stale_tel = (df.tel_age_runner > 2.0).mean()
    bw_err = df.bandwidth_err.mean()
    resid_avg = df.residual_bw.mean(); resid_p95 = df.residual_bw.quantile(0.95)
    q_avg = df.queue_len_after.mean(); q_p95 = df.queue_len_after.quantile(0.95)
    util_by_dev = df.groupby("device").util_before.mean().to_dict()
    counts = df.device.value_counts().values
    jain = (counts.sum()**2)/(len(counts)*(counts**2).sum())
    acc_by_dev = df.groupby("device").apply(lambda x:(x.pred_label==x.true_label).mean()).to_dict()
    acc_by_cls = df.groupby("true_label").apply(lambda x:(x.pred_label==x.true_label).mean()).to_dict()
    print("\n==== CORE ====")
    print(f"Accuracy {acc:.3f} | Macro-F1 {macro_f1:.3f} | NLL {nll:.3f} | ECE {ece:.3f}")
    print(f"Reward total {total_reward:.2f} (acc {df.r_acc.sum():.2f}, lat {df.r_lat.sum():.2f}, eng {df.r_eng.sum():.2f}, price {df.r_price.sum():.2f})")
    print("\n==== LATENCY & THROUGHPUT ====")
    print(f"T_total median {lat_med*1000:.1f}ms (P95 {lat_p95*1000:.1f}ms), T_tx median {tx_med*1000:.1f}ms (P95 {tx_p95*1000:.1f}ms)")
    print(f"Throughput {throughput:.2f} win/s | Deadline miss {miss_rate:.3f}")
    print("\n==== ENERGY & COST ====")
    print(f"Energy/window {e_per:.3f} J | Price/window {price_per:.3f} | Reward/£ {r_per_cost:.3f}")
    print("\n==== ROUTING & USAGE ====")
    print(f"Local {local_rate:.3f} | Device dist {dev_dist} | Slice dist {slice_dist} | Feasibility {feas_rate:.3f}")
    print("\n==== TELEMETRY & BANDWIDTH ====")
    print(f"Stale tel rate {stale_tel:.3f} | |b_meas - b_hat| {bw_err:.3f} | Residual avg {resid_avg:.2f} (P95 {resid_p95:.2f})")
    print("\n==== QUEUES & UTIL ====")
    print(f"Queue avg {q_avg:.2f} (P95 {q_p95:.2f}) | Util by device {util_by_dev}")
    print("\n==== FAIRNESS & ROBUSTNESS ====")
    print(f"Jain's index {jain:.3f} | Acc by device {acc_by_dev} | Acc by class {acc_by_cls}")
    return {"accuracy":acc, "macro_f1":macro_f1, "nll":nll, "ece":ece, "total_reward":total_reward, "jain":jain}

# ---------------- Main ----------------
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--data_dir", required=True)
    ap.add_argument("--artefact_dir", required=True)
    ap.add_argument("--out_dir", required=True)
    ap.add_argument("--n_devices", type=int, default=6)
    ap.add_argument("--alpha", type=float, default=5.0)
    ap.add_argument("--updates", type=int, default=10)
    ap.add_argument("--rollout_T", type=int, default=256)
    ap.add_argument("--hidden", type=int, default=64)
    ap.add_argument("--lr", type=float, default=3e-4)
    ap.add_argument("--deadline_s", type=float, default=0.25)
    ap.add_argument("--lambda_t", type=float, default=0.5)
    ap.add_argument("--lambda_e", type=float, default=0.2)
    ap.add_argument("--lambda_pi", type=float, default=0.2)
    ap.add_argument("--T0", type=float, default=0.20)
    ap.add_argument("--E0", type=float, default=0.20)
    ap.add_argument("--seed", type=int, default=42)
    args = ap.parse_args()

    os.makedirs(args.out_dir, exist_ok=True)
    np.random.seed(args.seed); random.seed(args.seed); torch.manual_seed(args.seed)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    per_art_path = os.path.join(args.artefact_dir, f"per_device_alpha{int(args.alpha)}.pkl")
    if os.path.exists(per_art_path):
        with open(per_art_path,"rb") as f: per_art = pickle.load(f)
        if not artefact_perdev_is_valid(per_art, args.n_devices):
            print("[per-device] artefact incompatible → rebuilding")
            per_art = build_per_device_artefact(args.data_dir, per_art_path, n_devices=args.n_devices, alpha=args.alpha, seed=args.seed)
    else:
        per_art = build_per_device_artefact(args.data_dir, per_art_path, n_devices=args.n_devices, alpha=args.alpha, seed=args.seed)

    Xtr, ytr, Xte, yte = load_uci_har_all(args.data_dir)
    test_assign = [np.array(arr, dtype=int) for arr in per_art["test_assignment"]]
    deques = [deque(list(idx_list)) for idx_list in test_assign]
    origin_sequence=[]
    while any(len(q)>0 for q in deques):
        for d in range(args.n_devices):
            if deques[d]: origin_sequence.append((d, deques[d].popleft()))
    origin_sequence = np.array(origin_sequence, dtype=object)

    clf = PerDeviceClassifier(per_art)
    env = HarRoutingEnvPerDev(origin_sequence, Xte, yte, clf=clf,
                              deadline_s=args.deadline_s, seed=args.seed,
                              lambda_t=args.lambda_t, lambda_e=args.lambda_e, lambda_pi=args.lambda_pi,
                              T0=args.T0, E0=args.E0)
    router = RouterGCN(in_dim=9, hidden=args.hidden, n_devices=args.n_devices, n_slices=len(R_CHOICES)).to(device)

    train_df = train_router(env, router, updates=args.updates, rollout_T=args.rollout_T, lr=args.lr)
    eval_df  = evaluate_budget_aware(env, latency_budget=args.deadline_s, max_steps=None)

    train_csv = os.path.join(args.out_dir, "train_metrics.csv")
    eval_csv  = os.path.join(args.out_dir, "window_log.csv")
    train_df.to_csv(train_csv, index=False); eval_df.to_csv(eval_csv, index=False)
    print("\n=== FILES SAVED ===")
    print(f"Train metrics: {train_csv}")
    print(f"Window log:    {eval_csv}")
    _ = compute_metrics(eval_df)

if __name__ == "__main__":
    main()

